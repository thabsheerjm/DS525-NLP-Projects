{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_df(file):\n",
    "    # initialize dictionary\n",
    "    diction = {'id':[],'questions':[],'context_id':[],'contexts':[],'answers':[]}\n",
    "    data = pd.read_json(file)\n",
    "    context_id = 0\n",
    "    for i in range(len(data)):\n",
    "        dp = data.iloc[[i]]\n",
    "        dt = dp.data[i]\n",
    "        title = dt['title']\n",
    "        para = dt['paragraphs']\n",
    "        for el in para:\n",
    "            context = el['context']\n",
    "            context_id+=1\n",
    "            qas = el['qas']\n",
    "            for qa in qas:\n",
    "                ans = qa['answers'][0]\n",
    "                answers = ans\n",
    "                question = qa['question']\n",
    "                id = qa['id']               \n",
    "                diction['id'].append(id)\n",
    "                diction['questions'].append(question)\n",
    "                diction['context_id'].append(context_id)\n",
    "                diction['contexts'].append(context)\n",
    "                diction['answers'].append(answers)   \n",
    "      \n",
    "    # dictionary to dataframe\n",
    "    df = pd.DataFrame(diction)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = extract_df(\"squad1.1/train-v1.1.json\")\n",
    "validation_df = extract_df(\"squad1.1/dev-v1.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 87599 questions in the training dataset\n",
      "There are 10570 questions in the validation dataset\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(training_df)} questions in the training dataset\")\n",
    "print(f\"There are {len(validation_df)} questions in the validation dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18891 unique contexts in the training dataset\n",
      "There are 2067 unique contexts in the validation dataset\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(training_df.contexts.unique())} unique contexts in the training dataset\")\n",
    "print(f\"There are {len(validation_df.contexts.unique())} unique contexts in the validation dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contexts, train_questions, train_answers = training_df['contexts'].to_list(),training_df['questions'].to_list(),training_df['answers'].to_list()\n",
    "val_contexts, val_questions, val_answers = validation_df['contexts'].to_list(),validation_df['questions'].to_list(),validation_df['answers'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        answer_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(answer_text)\n",
    "        if context[start_idx:end_idx] == answer_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        else:\n",
    "            for n in [1, 2]:\n",
    "                if context[start_idx-n:end_idx-n] == answer_text:\n",
    "                    answer['answer_start'] = start_idx - n\n",
    "                    answer['answer_end'] = end_idx - n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(val_answers, val_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        go_back = 1\n",
    "        while end_positions[-1] is None:\n",
    "            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end']-go_back)\n",
    "            go_back +=1\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "# apply function to our data\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(val_encodings, val_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForQuestionAnswering\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# initialize data loader for training data\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "    \n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        start_positions=start_positions,\n",
    "                        end_positions=end_positions)\n",
    "    \n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "     \n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "model_path = 'models/distilbert-custom'\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 661/661 [00:37<00:00, 17.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# switch model out of training mode\n",
    "model.eval()\n",
    "\n",
    "#val_sampler = SequentialSampler(val_dataset)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "acc = []\n",
    "predicted = []\n",
    "ground_truth = []\n",
    "# initialize loop for progress bar\n",
    "loop = tqdm(val_loader)\n",
    "# loop through batches\n",
    "for batch in loop:\n",
    "    # we don't need to calculate gradients as we're not training\n",
    "    with torch.no_grad():\n",
    "        # pull batched items from loader\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "        # make predictions\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        # pull preds out\n",
    "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "        # calculate accuracy for both and append to accuracy list\n",
    "        acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
    "        acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n",
    "        # predict the f1 and other metrics\n",
    "        predict = list(start_pred.cpu().numpy()+ end_pred.cpu().numpy())\n",
    "        gt = list(start_true.cpu().numpy() + end_true.cpu().numpy())\n",
    "        predicted.extend(predict)\n",
    "        ground_truth.extend(gt)\n",
    "# calculate average accuracy in total\n",
    "acc = sum(acc)/len(acc)\n",
    "bert_report =  metrics.classification_report(ground_truth,predicted)\n",
    "print(bert_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 64.19629349560672%\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy is {acc * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SQuAD2.0'''\n",
    "training_path = \"squad2.0/train-v2.0.json\"\n",
    "validation_path = \"squad2.0/dev-v2.0.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_squad(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    # initialize lists for contexts, questions, and answers\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    # iterate through all data in squad data\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                if 'plausible_answers' in qa.keys():\n",
    "                    access = 'plausible_answers'\n",
    "                else:\n",
    "                    access = 'answers'\n",
    "                for answer in qa['answers']:\n",
    "                    # append data to lists\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "    # return formatted data lists\n",
    "    return contexts, questions, answers\n",
    "\n",
    "\n",
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        answer_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(answer_text)\n",
    "        if context[start_idx:end_idx] == answer_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        else:\n",
    "            for n in [1, 2]:\n",
    "                if context[start_idx-n:end_idx-n] == answer_text:\n",
    "                   \n",
    "                    answer['answer_start'] = start_idx - n\n",
    "                    answer['answer_end'] = end_idx - n\n",
    "\n",
    "\n",
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "    \n",
    "        go_back = 1\n",
    "        while end_positions[-1] is None:\n",
    "            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end']-go_back)\n",
    "            go_back +=1\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contexts, train_questions, train_answers = read_squad(training_path)\n",
    "val_contexts, val_questions, val_answers = read_squad(validation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(val_answers, val_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to our data\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(val_encodings, val_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForQuestionAnswering\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thabsheerjm/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|██████████| 5427/5427 [15:33<00:00,  5.81it/s, loss=1.46] \n",
      "Epoch 1: 100%|██████████| 5427/5427 [15:32<00:00,  5.82it/s, loss=1.18] \n",
      "Epoch 2: 100%|██████████| 5427/5427 [15:32<00:00,  5.82it/s, loss=0.977]\n",
      "Epoch 3: 100%|██████████| 5427/5427 [15:35<00:00,  5.80it/s, loss=0.367] \n",
      "Epoch 4: 100%|██████████| 5427/5427 [15:33<00:00,  5.81it/s, loss=0.738] \n",
      "Epoch 5: 100%|██████████| 5427/5427 [15:34<00:00,  5.81it/s, loss=0.651]  \n",
      "Epoch 6: 100%|██████████| 5427/5427 [15:37<00:00,  5.79it/s, loss=0.121]  \n",
      "Epoch 7: 100%|██████████| 5427/5427 [15:38<00:00,  5.78it/s, loss=0.209]  \n",
      "Epoch 8: 100%|██████████| 5427/5427 [15:36<00:00,  5.80it/s, loss=0.326]  \n",
      "Epoch 9: 100%|██████████| 5427/5427 [15:36<00:00,  5.80it/s, loss=0.16]   \n",
      "Epoch 10: 100%|██████████| 5427/5427 [15:39<00:00,  5.77it/s, loss=0.14]   \n",
      "Epoch 11: 100%|██████████| 5427/5427 [15:33<00:00,  5.82it/s, loss=0.28]   \n",
      "Epoch 12: 100%|██████████| 5427/5427 [15:31<00:00,  5.82it/s, loss=0.0266] \n",
      "Epoch 13: 100%|██████████| 5427/5427 [15:31<00:00,  5.83it/s, loss=0.137]  \n",
      "Epoch 14: 100%|██████████| 5427/5427 [15:36<00:00,  5.80it/s, loss=0.301]  \n",
      "Epoch 15: 100%|██████████| 5427/5427 [15:36<00:00,  5.80it/s, loss=0.113]   \n",
      "Epoch 16: 100%|██████████| 5427/5427 [15:34<00:00,  5.81it/s, loss=0.0548]  \n",
      "Epoch 17: 100%|██████████| 5427/5427 [15:31<00:00,  5.83it/s, loss=0.0132]  \n",
      "Epoch 18: 100%|██████████| 5427/5427 [15:33<00:00,  5.81it/s, loss=0.0285]  \n",
      "Epoch 19: 100%|██████████| 5427/5427 [15:35<00:00,  5.80it/s, loss=0.00523] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/distilbert-custom/tokenizer_config.json',\n",
       " 'models/distilbert-custom/special_tokens_map.json',\n",
       " 'models/distilbert-custom/vocab.txt',\n",
       " 'models/distilbert-custom/added_tokens.json',\n",
       " 'models/distilbert-custom/tokenizer.json')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 5\n",
    "# setup GPU/CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# move model over to detected device\n",
    "model.to(device)\n",
    "# activate training mode of model\n",
    "model.train()\n",
    "# initialize adam optimizer with weight decay (reduces chance of overfitting)\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# initialize data loader for training data\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all the tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        # train model on batch and return outputs (incl. loss)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        start_positions=start_positions,\n",
    "                        end_positions=end_positions)\n",
    "        # extract loss\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "model_path = 'models/distilbert-custom'\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1269/1269 [01:10<00:00, 17.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.66      0.88      0.76        95\n",
      "           3       0.73      0.76      0.74        91\n",
      "           4       0.58      0.61      0.59       138\n",
      "           5       0.65      0.76      0.70       131\n",
      "           6       0.49      0.49      0.49       140\n",
      "           7       0.55      0.56      0.56        96\n",
      "           8       0.42      0.51      0.46        93\n",
      "           9       0.56      0.54      0.55       100\n",
      "          10       0.60      0.59      0.60        95\n",
      "          11       0.63      0.67      0.65        78\n",
      "          12       0.69      0.62      0.65        92\n",
      "          13       0.74      0.61      0.67        69\n",
      "          14       0.54      0.62      0.58       100\n",
      "          15       0.55      0.59      0.57        71\n",
      "          16       0.56      0.68      0.61       133\n",
      "          17       0.59      0.54      0.56       104\n",
      "          18       0.48      0.59      0.52        82\n",
      "          19       0.49      0.52      0.50        65\n",
      "          20       0.58      0.54      0.56       112\n",
      "          21       0.67      0.53      0.60        73\n",
      "          22       0.52      0.56      0.54       117\n",
      "          23       0.56      0.58      0.57        78\n",
      "          24       0.52      0.53      0.52        85\n",
      "          25       0.72      0.52      0.60        95\n",
      "          26       0.53      0.65      0.59       124\n",
      "          27       0.50      0.48      0.49       109\n",
      "          28       0.51      0.54      0.52        89\n",
      "          29       0.54      0.50      0.52       106\n",
      "          30       0.72      0.60      0.65       144\n",
      "          31       0.47      0.39      0.43       112\n",
      "          32       0.52      0.43      0.47        95\n",
      "          33       0.48      0.35      0.40       110\n",
      "          34       0.52      0.69      0.59       114\n",
      "          35       0.42      0.50      0.46        76\n",
      "          36       0.57      0.59      0.58       107\n",
      "          37       0.50      0.61      0.55       110\n",
      "          38       0.52      0.62      0.56       102\n",
      "          39       0.37      0.47      0.41        73\n",
      "          40       0.47      0.65      0.55        60\n",
      "          41       0.48      0.52      0.50        75\n",
      "          42       0.41      0.47      0.44        91\n",
      "          43       0.50      0.40      0.45        97\n",
      "          44       0.50      0.47      0.49       110\n",
      "          45       0.52      0.62      0.56        65\n",
      "          46       0.60      0.60      0.60       118\n",
      "          47       0.58      0.47      0.52        97\n",
      "          48       0.65      0.52      0.58       119\n",
      "          49       0.40      0.48      0.44        82\n",
      "          50       0.45      0.58      0.51        93\n",
      "          51       0.62      0.46      0.53       109\n",
      "          52       0.48      0.50      0.49        78\n",
      "          53       0.39      0.33      0.36        72\n",
      "          54       0.56      0.50      0.53        98\n",
      "          55       0.49      0.34      0.40        85\n",
      "          56       0.63      0.64      0.64       105\n",
      "          57       0.38      0.44      0.41        80\n",
      "          58       0.50      0.45      0.47       116\n",
      "          59       0.55      0.49      0.52        88\n",
      "          60       0.50      0.62      0.56        97\n",
      "          61       0.36      0.35      0.35        57\n",
      "          62       0.55      0.59      0.57       107\n",
      "          63       0.45      0.57      0.50        61\n",
      "          64       0.55      0.50      0.53        92\n",
      "          65       0.41      0.35      0.38        43\n",
      "          66       0.54      0.59      0.56        94\n",
      "          67       0.57      0.51      0.54        82\n",
      "          68       0.48      0.57      0.52        68\n",
      "          69       0.46      0.50      0.48        66\n",
      "          70       0.57      0.40      0.47       107\n",
      "          71       0.58      0.47      0.52        79\n",
      "          72       0.60      0.45      0.52        66\n",
      "          73       0.47      0.46      0.47        76\n",
      "          74       0.63      0.57      0.60        70\n",
      "          75       0.36      0.57      0.44        54\n",
      "          76       0.69      0.48      0.57        71\n",
      "          77       0.53      0.44      0.48        99\n",
      "          78       0.56      0.56      0.56        86\n",
      "          79       0.56      0.36      0.43        84\n",
      "          80       0.39      0.55      0.46        58\n",
      "          81       0.54      0.40      0.46        65\n",
      "          82       0.41      0.42      0.41        81\n",
      "          83       0.52      0.47      0.49        81\n",
      "          84       0.49      0.28      0.35        69\n",
      "          85       0.36      0.44      0.39        64\n",
      "          86       0.42      0.38      0.40        85\n",
      "          87       0.54      0.49      0.52        87\n",
      "          88       0.67      0.49      0.56       101\n",
      "          89       0.40      0.44      0.42        73\n",
      "          90       0.59      0.50      0.54        82\n",
      "          91       0.33      0.49      0.40        69\n",
      "          92       0.52      0.53      0.53        79\n",
      "          93       0.62      0.55      0.58        86\n",
      "          94       0.70      0.57      0.63        82\n",
      "          95       0.21      0.27      0.24        62\n",
      "          96       0.54      0.61      0.58        93\n",
      "          97       0.30      0.41      0.35        58\n",
      "          98       0.56      0.47      0.51        72\n",
      "          99       0.30      0.28      0.29        61\n",
      "         100       0.39      0.51      0.44        70\n",
      "         101       0.53      0.40      0.45        73\n",
      "         102       0.34      0.31      0.32        78\n",
      "         103       0.35      0.41      0.38        70\n",
      "         104       0.37      0.35      0.36        55\n",
      "         105       0.37      0.48      0.42        73\n",
      "         106       0.62      0.48      0.54        83\n",
      "         107       0.31      0.23      0.27        48\n",
      "         108       0.55      0.64      0.59        66\n",
      "         109       0.52      0.58      0.55        59\n",
      "         110       0.69      0.76      0.72        97\n",
      "         111       0.54      0.54      0.54        63\n",
      "         112       0.48      0.36      0.41       109\n",
      "         113       0.49      0.37      0.42        60\n",
      "         114       0.63      0.52      0.57        73\n",
      "         115       0.40      0.35      0.37        66\n",
      "         116       0.43      0.55      0.48        55\n",
      "         117       0.45      0.54      0.49        56\n",
      "         118       0.55      0.48      0.51        67\n",
      "         119       0.51      0.42      0.46        48\n",
      "         120       0.42      0.44      0.43        72\n",
      "         121       0.42      0.59      0.49        63\n",
      "         122       0.61      0.35      0.45       105\n",
      "         123       0.35      0.32      0.34        74\n",
      "         124       0.36      0.41      0.38        66\n",
      "         125       0.43      0.48      0.45        71\n",
      "         126       0.34      0.26      0.29        54\n",
      "         127       0.36      0.45      0.40        66\n",
      "         128       0.42      0.40      0.41        89\n",
      "         129       0.41      0.38      0.39        58\n",
      "         130       0.50      0.51      0.51        82\n",
      "         131       0.42      0.36      0.39        70\n",
      "         132       0.50      0.43      0.46        70\n",
      "         133       0.50      0.29      0.37        48\n",
      "         134       0.40      0.26      0.31        66\n",
      "         135       0.30      0.30      0.30        63\n",
      "         136       0.63      0.57      0.60        69\n",
      "         137       0.27      0.42      0.33        40\n",
      "         138       0.49      0.57      0.53        58\n",
      "         139       0.47      0.37      0.41        57\n",
      "         140       0.52      0.52      0.52        62\n",
      "         141       0.59      0.40      0.48        47\n",
      "         142       0.29      0.43      0.34        58\n",
      "         143       0.56      0.54      0.55        81\n",
      "         144       0.39      0.46      0.43        65\n",
      "         145       0.53      0.39      0.45        64\n",
      "         146       0.38      0.34      0.36        53\n",
      "         147       0.47      0.46      0.47        56\n",
      "         148       0.54      0.60      0.57        91\n",
      "         149       0.44      0.44      0.44        54\n",
      "         150       0.40      0.34      0.37        58\n",
      "         151       0.45      0.52      0.48        62\n",
      "         152       0.48      0.32      0.38        66\n",
      "         153       0.51      0.49      0.50        70\n",
      "         154       0.58      0.34      0.43        53\n",
      "         155       0.45      0.43      0.44        42\n",
      "         156       0.52      0.58      0.55        72\n",
      "         157       0.37      0.44      0.40        43\n",
      "         158       0.49      0.40      0.44        47\n",
      "         159       0.54      0.46      0.50        63\n",
      "         160       0.60      0.37      0.46        67\n",
      "         161       0.41      0.46      0.43        61\n",
      "         162       0.58      0.53      0.55        62\n",
      "         163       0.30      0.30      0.30        40\n",
      "         164       0.37      0.44      0.40        52\n",
      "         165       0.61      0.51      0.55        61\n",
      "         166       0.51      0.53      0.52        73\n",
      "         167       0.43      0.49      0.46        79\n",
      "         168       0.60      0.40      0.48        78\n",
      "         169       0.31      0.38      0.34        58\n",
      "         170       0.41      0.49      0.45        65\n",
      "         171       0.44      0.71      0.54        48\n",
      "         172       0.52      0.54      0.53        70\n",
      "         173       0.32      0.20      0.24        41\n",
      "         174       0.58      0.49      0.53        67\n",
      "         175       0.58      0.62      0.60        76\n",
      "         176       0.76      0.51      0.61        63\n",
      "         177       0.57      0.45      0.50        62\n",
      "         178       0.58      0.57      0.57        53\n",
      "         179       0.46      0.61      0.53        62\n",
      "         180       0.45      0.56      0.50        54\n",
      "         181       0.42      0.56      0.48        43\n",
      "         182       0.30      0.42      0.35        52\n",
      "         183       0.57      0.44      0.50        59\n",
      "         184       0.50      0.55      0.52        84\n",
      "         185       0.43      0.51      0.47        45\n",
      "         186       0.48      0.50      0.49        42\n",
      "         187       0.48      0.50      0.49        58\n",
      "         188       0.52      0.49      0.50        59\n",
      "         189       0.39      0.39      0.39        33\n",
      "         190       0.44      0.39      0.41        57\n",
      "         191       0.45      0.43      0.44        70\n",
      "         192       0.41      0.39      0.40        72\n",
      "         193       0.33      0.50      0.40        42\n",
      "         194       0.56      0.49      0.52        51\n",
      "         195       0.62      0.50      0.55        42\n",
      "         196       0.67      0.50      0.57        76\n",
      "         197       0.44      0.56      0.49        61\n",
      "         198       0.54      0.49      0.51        57\n",
      "         199       0.39      0.45      0.42        44\n",
      "         200       0.38      0.45      0.41        42\n",
      "         201       0.29      0.52      0.37        27\n",
      "         202       0.58      0.43      0.49        42\n",
      "         203       0.55      0.60      0.58        63\n",
      "         204       0.44      0.51      0.47        41\n",
      "         205       0.57      0.63      0.60        54\n",
      "         206       0.33      0.40      0.36        48\n",
      "         207       0.26      0.53      0.35        30\n",
      "         208       0.39      0.51      0.44        47\n",
      "         209       0.36      0.26      0.30        47\n",
      "         210       0.63      0.81      0.71        70\n",
      "         211       0.42      0.46      0.44        35\n",
      "         212       0.30      0.24      0.27        41\n",
      "         213       0.60      0.41      0.49        66\n",
      "         214       0.23      0.26      0.25        34\n",
      "         215       0.69      0.50      0.58        58\n",
      "         216       0.46      0.59      0.52        29\n",
      "         217       0.35      0.36      0.35        42\n",
      "         218       0.47      0.34      0.40        70\n",
      "         219       0.36      0.35      0.35        43\n",
      "         220       0.52      0.54      0.53        56\n",
      "         221       0.36      0.60      0.45        40\n",
      "         222       0.41      0.55      0.47        44\n",
      "         223       0.52      0.54      0.53        41\n",
      "         224       0.40      0.59      0.47        56\n",
      "         225       0.51      0.42      0.46        62\n",
      "         226       0.33      0.40      0.36        43\n",
      "         227       0.29      0.36      0.32        36\n",
      "         228       0.62      0.45      0.53        33\n",
      "         229       0.59      0.62      0.60        26\n",
      "         230       0.67      0.58      0.62        52\n",
      "         231       0.67      0.58      0.62        64\n",
      "         232       0.38      0.49      0.43        45\n",
      "         233       0.39      0.38      0.39        29\n",
      "         234       0.36      0.38      0.37        45\n",
      "         235       0.35      0.68      0.46        25\n",
      "         236       0.30      0.36      0.33        39\n",
      "         237       0.51      0.56      0.54        32\n",
      "         238       0.26      0.43      0.33        21\n",
      "         239       0.68      0.83      0.75        41\n",
      "         240       0.17      0.50      0.26        14\n",
      "         241       0.67      0.22      0.33        27\n",
      "         242       0.44      0.40      0.41        43\n",
      "         243       0.50      0.44      0.47        39\n",
      "         244       0.35      0.42      0.38        43\n",
      "         245       0.62      0.52      0.56        31\n",
      "         246       0.59      0.58      0.58        45\n",
      "         247       0.43      0.49      0.46        39\n",
      "         248       0.64      0.58      0.61        36\n",
      "         249       0.42      0.45      0.43        47\n",
      "         250       0.78      0.56      0.65        75\n",
      "         251       0.42      0.43      0.43        37\n",
      "         252       0.14      0.14      0.14        21\n",
      "         253       0.43      0.45      0.44        20\n",
      "         254       0.47      0.51      0.49        41\n",
      "         255       0.38      0.29      0.33        31\n",
      "         256       0.47      0.39      0.43        23\n",
      "         257       0.14      0.16      0.15        19\n",
      "         258       0.50      0.61      0.55        33\n",
      "         259       0.25      0.62      0.36        21\n",
      "         260       0.57      0.74      0.64        23\n",
      "         261       0.74      0.47      0.57        43\n",
      "         262       0.22      0.39      0.28        18\n",
      "         263       0.44      0.38      0.40        45\n",
      "         264       0.22      0.33      0.27        33\n",
      "         265       0.35      0.38      0.36        24\n",
      "         266       0.33      0.47      0.39        19\n",
      "         267       0.46      0.40      0.42        43\n",
      "         268       0.52      0.48      0.50        33\n",
      "         269       0.34      0.46      0.39        24\n",
      "         270       0.79      0.30      0.43        37\n",
      "         271       1.00      0.16      0.27        19\n",
      "         272       0.46      0.46      0.46        26\n",
      "         273       0.64      0.74      0.68        34\n",
      "         274       0.13      0.43      0.20         7\n",
      "         275       0.29      0.35      0.31        23\n",
      "         276       0.84      0.52      0.64        31\n",
      "         277       0.36      0.32      0.34        31\n",
      "         278       0.30      0.35      0.32        23\n",
      "         279       0.50      0.48      0.49        23\n",
      "         280       0.29      0.30      0.30        23\n",
      "         281       0.45      0.43      0.44        21\n",
      "         282       0.60      0.55      0.58        38\n",
      "         283       0.56      0.48      0.52        31\n",
      "         284       0.59      0.48      0.53        33\n",
      "         285       0.72      0.51      0.60        41\n",
      "         286       0.41      0.68      0.51        19\n",
      "         287       0.78      0.47      0.58        15\n",
      "         288       0.29      0.46      0.35        13\n",
      "         289       0.48      0.50      0.49        30\n",
      "         290       0.40      0.50      0.44        16\n",
      "         291       0.33      0.50      0.40        18\n",
      "         292       0.75      0.54      0.63        28\n",
      "         293       0.81      0.78      0.79        27\n",
      "         294       0.09      0.10      0.10        10\n",
      "         295       0.33      0.18      0.24        11\n",
      "         296       0.14      0.14      0.14         7\n",
      "         297       0.22      0.53      0.31        15\n",
      "         298       0.25      0.29      0.27        24\n",
      "         299       0.65      0.37      0.47        30\n",
      "         300       0.83      0.56      0.67        27\n",
      "         301       1.00      0.45      0.62        11\n",
      "         302       0.55      0.50      0.52        22\n",
      "         303       0.75      0.45      0.56        20\n",
      "         304       0.19      0.38      0.25        13\n",
      "         305       0.14      0.08      0.11        24\n",
      "         306       0.58      0.54      0.56        26\n",
      "         307       0.24      0.36      0.29        11\n",
      "         308       0.57      0.55      0.56        29\n",
      "         309       0.00      0.00      0.00         6\n",
      "         310       0.45      0.68      0.55        22\n",
      "         311       0.59      0.62      0.61        16\n",
      "         312       0.14      0.07      0.10        14\n",
      "         313       0.43      0.19      0.26        16\n",
      "         314       0.25      0.25      0.25        12\n",
      "         315       0.00      0.00      0.00         6\n",
      "         316       0.33      0.39      0.36        18\n",
      "         317       0.33      0.17      0.22         6\n",
      "         318       1.00      0.62      0.76        13\n",
      "         319       0.55      0.24      0.33        25\n",
      "         320       0.31      0.33      0.32        12\n",
      "         321       0.67      0.57      0.62         7\n",
      "         322       0.56      0.71      0.63        14\n",
      "         323       0.33      0.42      0.37        12\n",
      "         324       0.38      0.60      0.46        20\n",
      "         325       0.33      0.11      0.16        19\n",
      "         326       0.67      0.76      0.71        21\n",
      "         327       0.17      0.33      0.22         6\n",
      "         328       0.58      0.44      0.50        16\n",
      "         329       0.38      0.20      0.26        15\n",
      "         330       0.06      0.11      0.08         9\n",
      "         331       0.00      0.00      0.00         8\n",
      "         332       0.48      0.50      0.49        26\n",
      "         333       0.00      0.00      0.00        10\n",
      "         334       0.45      0.71      0.56         7\n",
      "         335       0.58      0.54      0.56        13\n",
      "         336       0.12      0.15      0.14        13\n",
      "         337       0.74      0.74      0.74        19\n",
      "         338       0.36      0.56      0.43         9\n",
      "         339       0.83      0.38      0.53        13\n",
      "         340       0.64      0.41      0.50        22\n",
      "         341       0.50      0.55      0.52        11\n",
      "         342       0.62      0.62      0.62         8\n",
      "         343       0.44      0.20      0.28        20\n",
      "         344       0.17      0.15      0.16        13\n",
      "         345       0.71      0.31      0.43        16\n",
      "         346       0.44      0.50      0.47        16\n",
      "         347       0.00      0.00      0.00        13\n",
      "         348       0.70      0.56      0.62        25\n",
      "         349       0.71      0.62      0.67         8\n",
      "         350       0.00      0.00      0.00         1\n",
      "         351       0.33      0.29      0.31         7\n",
      "         352       0.19      0.43      0.26         7\n",
      "         353       0.70      0.58      0.64        12\n",
      "         354       0.73      0.42      0.53        19\n",
      "         355       0.50      1.00      0.67         3\n",
      "         356       0.30      0.64      0.41        11\n",
      "         357       0.29      0.50      0.36         8\n",
      "         358       0.73      0.57      0.64        14\n",
      "         359       0.50      0.43      0.46         7\n",
      "         360       0.43      0.40      0.41        15\n",
      "         361       0.64      0.39      0.48        18\n",
      "         362       0.28      0.50      0.36        10\n",
      "         363       0.00      0.00      0.00         5\n",
      "         364       0.62      0.56      0.59         9\n",
      "         365       0.83      0.77      0.80        13\n",
      "         366       0.50      0.50      0.50         6\n",
      "         367       0.00      0.00      0.00         7\n",
      "         368       0.56      0.56      0.56         9\n",
      "         369       0.25      0.25      0.25         8\n",
      "         370       0.22      0.50      0.31        12\n",
      "         371       0.67      0.70      0.68        20\n",
      "         372       0.55      0.46      0.50        13\n",
      "         373       0.00      0.00      0.00         3\n",
      "         374       0.44      0.44      0.44         9\n",
      "         375       0.00      0.00      0.00         6\n",
      "         376       0.55      0.86      0.67         7\n",
      "         377       0.00      0.00      0.00        11\n",
      "         378       0.50      0.23      0.32        13\n",
      "         379       0.29      0.17      0.21        12\n",
      "         380       0.50      0.39      0.44        18\n",
      "         381       0.50      0.75      0.60         4\n",
      "         382       0.00      0.00      0.00         4\n",
      "         383       0.33      0.33      0.33         3\n",
      "         384       0.00      0.00      0.00         2\n",
      "         385       1.00      0.86      0.92         7\n",
      "         386       0.00      0.00      0.00         4\n",
      "         387       0.62      0.45      0.53        11\n",
      "         388       1.00      0.43      0.60        14\n",
      "         389       1.00      0.19      0.32        16\n",
      "         390       0.38      0.33      0.35         9\n",
      "         391       0.67      0.60      0.63        10\n",
      "         392       0.00      0.00      0.00         6\n",
      "         393       0.67      0.73      0.70        11\n",
      "         394       0.00      0.00      0.00         1\n",
      "         395       0.60      0.50      0.55        12\n",
      "         396       0.00      0.00      0.00         1\n",
      "         397       0.00      0.00      0.00         2\n",
      "         398       0.00      0.00      0.00         1\n",
      "         399       0.00      0.00      0.00         3\n",
      "         400       0.00      0.00      0.00         2\n",
      "         401       0.00      0.00      0.00         3\n",
      "         402       0.00      0.00      0.00         1\n",
      "         403       0.18      0.50      0.27         4\n",
      "         404       0.00      0.00      0.00         3\n",
      "         405       0.40      0.40      0.40        10\n",
      "         406       0.38      0.75      0.50         8\n",
      "         407       1.00      1.00      1.00         3\n",
      "         408       0.00      0.00      0.00         3\n",
      "         409       0.67      0.75      0.71         8\n",
      "         410       0.33      0.12      0.18         8\n",
      "         411       0.50      0.50      0.50         8\n",
      "         412       0.50      0.64      0.56        11\n",
      "         413       0.67      0.36      0.47        11\n",
      "         414       0.00      0.00      0.00         3\n",
      "         415       0.00      0.00      0.00         4\n",
      "         416       0.00      0.00      0.00         2\n",
      "         417       1.00      0.43      0.60         7\n",
      "         418       0.00      0.00      0.00         2\n",
      "         419       0.00      0.00      0.00         3\n",
      "         420       1.00      0.43      0.60         7\n",
      "         421       0.30      0.50      0.37         6\n",
      "         422       0.69      0.90      0.78        10\n",
      "         423       0.44      0.44      0.44         9\n",
      "         424       1.00      1.00      1.00         3\n",
      "         425       1.00      0.75      0.86         4\n",
      "         426       0.00      0.00      0.00         2\n",
      "         427       0.33      1.00      0.50         1\n",
      "         428       1.00      0.67      0.80         9\n",
      "         429       0.33      0.25      0.29         4\n",
      "         430       0.67      0.75      0.71         8\n",
      "         431       0.33      1.00      0.50         1\n",
      "         432       0.62      0.38      0.48        13\n",
      "         433       1.00      1.00      1.00         5\n",
      "         434       0.38      0.71      0.50         7\n",
      "         437       0.00      0.00      0.00         2\n",
      "         438       1.00      1.00      1.00         3\n",
      "         439       0.00      0.00      0.00         1\n",
      "         440       0.00      0.00      0.00         5\n",
      "         441       0.00      0.00      0.00         1\n",
      "         442       0.50      0.43      0.46         7\n",
      "         443       0.00      0.00      0.00         1\n",
      "         444       0.00      0.00      0.00         4\n",
      "         445       0.00      0.00      0.00         1\n",
      "         446       1.00      0.89      0.94         9\n",
      "         447       1.00      0.62      0.77         8\n",
      "         449       0.50      0.60      0.55         5\n",
      "         450       0.00      0.00      0.00         0\n",
      "         451       0.89      1.00      0.94         8\n",
      "         452       0.75      1.00      0.86         3\n",
      "         453       0.00      0.00      0.00         3\n",
      "         454       0.67      1.00      0.80         2\n",
      "         456       1.00      0.80      0.89        10\n",
      "         457       0.44      0.40      0.42        10\n",
      "         458       0.33      0.25      0.29         8\n",
      "         459       0.67      0.67      0.67         3\n",
      "         460       0.50      0.75      0.60         4\n",
      "         461       0.00      0.00      0.00         1\n",
      "         462       0.38      0.60      0.46         5\n",
      "         463       0.00      0.00      0.00         4\n",
      "         464       0.14      0.50      0.22         2\n",
      "         465       0.80      0.67      0.73         6\n",
      "         468       0.00      0.00      0.00         1\n",
      "         469       0.00      0.00      0.00         2\n",
      "         471       0.00      0.00      0.00         0\n",
      "         472       0.00      0.00      0.00         0\n",
      "         473       0.00      0.00      0.00         1\n",
      "         474       0.00      0.00      0.00         2\n",
      "         475       0.00      0.00      0.00         2\n",
      "         476       1.00      0.50      0.67         8\n",
      "         477       0.57      0.67      0.62         6\n",
      "         478       0.70      0.78      0.74         9\n",
      "         479       0.50      0.43      0.46         7\n",
      "         480       0.00      0.00      0.00         1\n",
      "         481       0.00      0.00      0.00         2\n",
      "         482       0.00      0.00      0.00         2\n",
      "         483       0.50      1.00      0.67         3\n",
      "         484       0.17      1.00      0.29         1\n",
      "         486       1.00      1.00      1.00         4\n",
      "         487       0.38      0.75      0.50         4\n",
      "         489       0.17      1.00      0.29         1\n",
      "         490       0.20      0.33      0.25         3\n",
      "         491       0.50      0.75      0.60         4\n",
      "         492       0.00      0.00      0.00         3\n",
      "         493       0.00      0.00      0.00         3\n",
      "         495       0.67      0.67      0.67         3\n",
      "         496       0.67      1.00      0.80         2\n",
      "         497       0.00      0.00      0.00         1\n",
      "         499       0.00      0.00      0.00         6\n",
      "         500       0.67      0.50      0.57         4\n",
      "         502       0.00      0.00      0.00         0\n",
      "         503       1.00      1.00      1.00         3\n",
      "         504       0.00      0.00      0.00         0\n",
      "         505       0.00      0.00      0.00         2\n",
      "         507       0.25      0.33      0.29         3\n",
      "         508       1.00      0.60      0.75         5\n",
      "         509       0.00      0.00      0.00         2\n",
      "         511       1.00      0.62      0.76        13\n",
      "         512       0.00      0.00      0.00         1\n",
      "         513       0.43      0.60      0.50         5\n",
      "         515       0.00      0.00      0.00         5\n",
      "         516       0.60      0.82      0.69        11\n",
      "         517       0.00      0.00      0.00         2\n",
      "         518       0.00      0.00      0.00         4\n",
      "         519       0.62      1.00      0.77         5\n",
      "         520       0.00      0.00      0.00         4\n",
      "         521       0.25      0.50      0.33         2\n",
      "         523       0.00      0.00      0.00         5\n",
      "         524       0.20      1.00      0.33         1\n",
      "         525       0.00      0.00      0.00         4\n",
      "         526       0.00      0.00      0.00         1\n",
      "         528       0.83      1.00      0.91         5\n",
      "         529       0.00      0.00      0.00         1\n",
      "         532       0.33      1.00      0.50         3\n",
      "         533       0.00      0.00      0.00         1\n",
      "         534       0.00      0.00      0.00         5\n",
      "         537       0.67      0.50      0.57         4\n",
      "         538       0.00      0.00      0.00         2\n",
      "         539       1.00      1.00      1.00         3\n",
      "         544       1.00      1.00      1.00         3\n",
      "         548       0.67      0.33      0.44         6\n",
      "         549       0.00      0.00      0.00         1\n",
      "         552       0.00      0.00      0.00         0\n",
      "         553       0.00      0.00      0.00         1\n",
      "         555       0.00      0.00      0.00         2\n",
      "         558       0.67      0.67      0.67         3\n",
      "         562       0.80      1.00      0.89         4\n",
      "         565       0.00      0.00      0.00         4\n",
      "         567       0.00      0.00      0.00         1\n",
      "         568       0.88      0.70      0.78        10\n",
      "         573       0.67      1.00      0.80         2\n",
      "         575       1.00      0.75      0.86         4\n",
      "         576       1.00      0.70      0.82        10\n",
      "         577       0.00      0.00      0.00         1\n",
      "         582       0.00      0.00      0.00         2\n",
      "         583       0.00      0.00      0.00         1\n",
      "         584       0.00      0.00      0.00         0\n",
      "         589       0.00      0.00      0.00         3\n",
      "         590       0.00      0.00      0.00         2\n",
      "         592       0.33      1.00      0.50         1\n",
      "         596       0.00      0.00      0.00         4\n",
      "         597       0.22      1.00      0.36         2\n",
      "         598       0.00      0.00      0.00         0\n",
      "         600       0.00      0.00      0.00         3\n",
      "         603       0.00      0.00      0.00         2\n",
      "         604       0.00      0.00      0.00         1\n",
      "         608       0.00      0.00      0.00         5\n",
      "         609       0.33      1.00      0.50         1\n",
      "         612       0.00      0.00      0.00         2\n",
      "         613       0.00      0.00      0.00         2\n",
      "         615       0.00      0.00      0.00         0\n",
      "         624       0.00      0.00      0.00         1\n",
      "         628       0.67      1.00      0.80         2\n",
      "         630       0.00      0.00      0.00         1\n",
      "         631       0.00      0.00      0.00         1\n",
      "         634       0.00      0.00      0.00         1\n",
      "         637       0.00      0.00      0.00         2\n",
      "         638       0.00      0.00      0.00         3\n",
      "         639       0.75      1.00      0.86         3\n",
      "         646       0.00      0.00      0.00         3\n",
      "         650       1.00      1.00      1.00         2\n",
      "         653       0.00      0.00      0.00         0\n",
      "         655       0.67      1.00      0.80         2\n",
      "         657       0.00      0.00      0.00         0\n",
      "         658       0.00      0.00      0.00         3\n",
      "         663       0.00      0.00      0.00         3\n",
      "         664       0.25      1.00      0.40         1\n",
      "         671       0.00      0.00      0.00         1\n",
      "         672       0.00      0.00      0.00         4\n",
      "         677       0.00      0.00      0.00         1\n",
      "         678       0.00      0.00      0.00         1\n",
      "         679       0.00      0.00      0.00         2\n",
      "         681       0.00      0.00      0.00         1\n",
      "         683       0.00      0.00      0.00         0\n",
      "         684       0.00      0.00      0.00         0\n",
      "         686       0.00      0.00      0.00         0\n",
      "         690       0.00      0.00      0.00         0\n",
      "         701       0.00      0.00      0.00         4\n",
      "         732       0.00      0.00      0.00         2\n",
      "         734       0.00      0.00      0.00         2\n",
      "         736       0.00      0.00      0.00         1\n",
      "         740       0.00      0.00      0.00         1\n",
      "         743       0.00      0.00      0.00         0\n",
      "         752       0.00      0.00      0.00         1\n",
      "         764       1.00      1.00      1.00         6\n",
      "         765       0.00      0.00      0.00         0\n",
      "         768       0.00      0.00      0.00         2\n",
      "         775       0.00      0.00      0.00         3\n",
      "         776       0.00      0.00      0.00         0\n",
      "         779       0.00      0.00      0.00         0\n",
      "         780       0.00      0.00      0.00         2\n",
      "         782       0.00      0.00      0.00         3\n",
      "         783       0.00      0.00      0.00         2\n",
      "         784       0.00      0.00      0.00         0\n",
      "         788       0.00      0.00      0.00         0\n",
      "         812       0.00      0.00      0.00         3\n",
      "         821       0.00      0.00      0.00         3\n",
      "         844       0.00      0.00      0.00         0\n",
      "         849       1.00      1.00      1.00         3\n",
      "         887       0.00      0.00      0.00         3\n",
      "         900       0.00      0.00      0.00         1\n",
      "         904       0.00      0.00      0.00         0\n",
      "         905       1.00      0.57      0.73         7\n",
      "         911       0.00      0.00      0.00         5\n",
      "         912       0.00      0.00      0.00         1\n",
      "         915       0.00      0.00      0.00         1\n",
      "         925       1.00      1.00      1.00         3\n",
      "         993       0.00      0.00      0.00         3\n",
      "         994       0.00      0.00      0.00         3\n",
      "         998       0.00      0.00      0.00         0\n",
      "        1002       0.00      0.00      0.00         3\n",
      "        1010       0.00      0.00      0.00         3\n",
      "        1011       0.00      0.00      0.00         1\n",
      "        1016       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.49     20302\n",
      "   macro avg       0.39      0.40      0.38     20302\n",
      "weighted avg       0.50      0.49      0.49     20302\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/thabsheerjm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/thabsheerjm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/thabsheerjm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/thabsheerjm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/thabsheerjm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/thabsheerjm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# switch model out of training mode\n",
    "model.eval()\n",
    "\n",
    "#val_sampler = SequentialSampler(val_dataset)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "acc = []\n",
    "predicted = []\n",
    "ground_truth = []\n",
    "# initialize loop for progress bar\n",
    "loop = tqdm(val_loader)\n",
    "# loop through batches\n",
    "for batch in loop:\n",
    "    # we don't need to calculate gradients as we're not training\n",
    "    with torch.no_grad():\n",
    "        # pull batched items from loader\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "        # make predictions\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        # pull preds out\n",
    "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "        # calculate accuracy for both and append to accuracy list\n",
    "        acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
    "        acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n",
    "        # predict the f1 and other metrics\n",
    "        predict = list(start_pred.cpu().numpy()+ end_pred.cpu().numpy())\n",
    "        gt = list(start_true.cpu().numpy() + end_true.cpu().numpy())\n",
    "        predicted.extend(predict)\n",
    "        ground_truth.extend(gt)\n",
    "# calculate average accuracy in total\n",
    "acc = sum(acc)/len(acc)\n",
    "bert_report =  metrics.classification_report(ground_truth,predicted)\n",
    "print(bert_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
